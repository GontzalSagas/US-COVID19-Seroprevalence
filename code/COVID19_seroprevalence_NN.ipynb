{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98d4468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "from math import trunc\n",
    "from datetime import datetime, date, timedelta\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e38ad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '../data/aggregates/'\n",
    "\n",
    "# CTIS data collected by CMU and Facebook:\n",
    "US_ML = pd.concat([pd.read_csv(os.path.join(directory, filename)) for filename in os.listdir(directory) if os.path.isfile(os.path.join(directory, filename))])\n",
    "\n",
    "regions1 = US_ML.state.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a358a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth of serology data from CDC:\n",
    "directory1 = '../data/'\n",
    "US_official = pd.read_csv(os.path.join(directory1, \"Nationwide_Commercial_Laboratory_Seroprevalence_Survey.csv\"))\n",
    "\n",
    "# NaNs in the database are represented by 777. We remove the empty rows.\n",
    "US_official['Rate (%) [All Ages Cumulative Prevalence, Rounds 1-30 only]'] = US_official['Rate (%) [All Ages Cumulative Prevalence, Rounds 1-30 only]'].replace(777, np.nan)\n",
    "US_official.drop(US_official[US_official['Round']>30].index,inplace=True)\n",
    "\n",
    "US_official.drop(US_official[US_official['Estimated cumulative infections count'].isna()].index,inplace=True)\n",
    "US_official['Estimated cumulative infections count'] = US_official['Estimated cumulative infections count'].apply(lambda x: int(x.replace(\",\",\"\")))\n",
    "\n",
    "regions2 = US_official.Site.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a85f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_date(date_string, start_end):\n",
    "    if len(date_string) > 25:\n",
    "        start_month_day_year, end_month_day_year =  list(map(lambda x: x.strip(), date_string.split(' - ')))\n",
    "        start_month_day, start_year = start_month_day_year.split(', ')\n",
    "        end_month_day, end_year = end_month_day_year.split(', ')\n",
    "        start_date = datetime.strptime(start_month_day + ' ' + start_year,'%b %d %Y')\n",
    "        end_date = datetime.strptime(end_month_day + ' ' + end_year,'%b %d %Y')\n",
    "    else:\n",
    "        month_day, year = date_string.split(', ')\n",
    "        start_month_day, end_month_day = list(map(lambda x: x.strip(), month_day.split(' - ')))\n",
    "        start_date = datetime.strptime(start_month_day + ' ' + year, '%b %d %Y')\n",
    "        end_date = datetime.strptime(end_month_day + ' ' + year, '%b %d %Y')\n",
    "    if start_end == 'start':\n",
    "        return start_date.strftime('%Y-%m-%d')\n",
    "\n",
    "    else:\n",
    "        return end_date.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8f569f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save start and end dates of rounds:\n",
    "US_official['start_date'] = US_official['Date Range of Specimen Collection'].map(lambda x: format_date(x, 'start'), na_action='ignore')\n",
    "US_official['end_date'] = US_official['Date Range of Specimen Collection'].map(lambda x: format_date(x, 'end'), na_action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58edba2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "US_official['days between rounds'] = US_official['end_date'].map(lambda x: datetime.strptime(x, '%Y-%m-%d') if (type(x)== str) else timedelta(days=14)) - US_official['start_date'].map(lambda x: datetime.strptime(x, '%Y-%m-%d') if (type(x)== str) else timedelta(days=0))\n",
    "US_official['days between rounds'] = US_official['days between rounds'].map(lambda x: x.days + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d20f9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Official data from Our World in Data:\n",
    "US_cases = pd.read_csv(os.path.join(directory1, \"United_States_COVID-19_Cases_and_Deaths_by_State_over_Time.csv\"))\n",
    "\n",
    "#We adapt the date to the format we are using (yyyy-mm-dd):\n",
    "US_cases['submission_date'] = US_cases['submission_date'].apply(lambda x: datetime.strptime(x, '%m/%d/%Y'))\n",
    "\n",
    "US_cases['new_case'] = US_cases['new_case'].apply(lambda x: int(x.replace(\",\",\"\")))\n",
    "US_cases['tot_death'] = US_cases['tot_death'].apply(lambda x: int(x.replace(\",\",\"\")))\n",
    "\n",
    "regions3 = US_cases.state.unique()\n",
    "\n",
    "# Example to see data: Arkansas\n",
    "data = US_cases[US_cases['state'] == 'AK'].sort_values(by='submission_date') #Take data chronologically\n",
    "plt.plot( data['submission_date'], data['new_case']) #Plot of total cases in Arkansas\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5fac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_alphaCode sets the codes used to identify the states:\n",
    "states = pd.read_csv(os.path.join(directory1, \"State_codes.csv\"))\n",
    "states = states.drop(columns=['Numeric Code  '], axis=1)\n",
    "states = states.rename(columns={' Name ' : 'State', ' Alpha Code ': 'Alpha Code'})\n",
    "states.State = states.State.apply(lambda s: s.strip().capitalize())\n",
    "states_unformatted = states.to_dict('records')\n",
    "state_alphaCode = dict()\n",
    "\n",
    "for unformatted_element in states_unformatted:\n",
    "    state_alphaCode[unformatted_element['State']] = unformatted_element['Alpha Code'].strip()\n",
    "    \n",
    "# Add state codes in US_ML:\n",
    "US_ML['state code']  = US_ML['state'].map(lambda x: x.capitalize()).replace(state_alphaCode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35054d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definite list of states to be used:\n",
    "regions = list(US_ML['state code'].unique())\n",
    "regions.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7e7835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COVID-19 cases estimations via wastewater SARS-CoV-2 concentration.\n",
    "# WW_cases is the estimation for total infected population throughout time\n",
    "WW_cases = pd.read_csv(os.path.join(directory1, \"ww_estimate_infections.csv\"))\n",
    "WW_cases.drop(labels='id',axis=1,inplace=True) #We drop the states' id\n",
    "\n",
    "WW_cases['state code']  = WW_cases['Country'].map(lambda x: x.capitalize()).replace(state_alphaCode) #adds the state code\n",
    "#There are colonies, not only states and DC. remove the colonies:\n",
    "for state_i in WW_cases['state code'].unique():\n",
    "    if state_i not in state_alphaCode.values():\n",
    "        WW_cases.drop(WW_cases[WW_cases['state code']==state_i].index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec73d9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "WW_new = pd.DataFrame(columns=['site','date','cases'])\n",
    "dates_WW = WW_cases.columns[1:-1]\n",
    "counter = 0\n",
    "for state in WW_cases['state code'].unique():\n",
    "    WW_new.loc[counter] = [state,dates_WW[0],WW_cases[WW_cases['state code']==state][dates_WW[0]].iloc[0]]\n",
    "    counter += 1\n",
    "    for date_i in range(1,len(dates_WW)):\n",
    "        WW_new.loc[counter] = [state,dates_WW[date_i],WW_cases[WW_cases['state code']==state][dates_WW[date_i]].iloc[0]]\n",
    "        counter += 1\n",
    "WW_new.rename(columns={'cases': 'ww_cases'}, inplace=True) #rename \"cases\" so that it's not confused with other data\n",
    "WW_cases = WW_new\n",
    "#Now we have the same format as in the other dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47001da",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9f66d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AcumulatedIncidences:\n",
    "\n",
    "    \"\"\"dataframes to be used\"\"\"\n",
    "    US_ML = US_ML\n",
    "    US_official = US_official\n",
    "    US_cases = US_cases\n",
    "    WW_cases = WW_cases\n",
    "    phi = (1+math.sqrt(5))/2 # for the purpose of making graph look nice\n",
    "    regions = list(US_ML['state code'].unique()) #given all datasets have rows associated to these states, we will use these\n",
    "    #Remember: regions=list(US_ML['state code'].unique())\n",
    "\n",
    "    \"\"\"\n",
    "    define a start_date and an end_date to define the interval of study\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self,signals=['p_cli','p_rf','p_XGB','p_glm','new_case','ww_cases']):\n",
    "        #signals are explanatory variables\n",
    "        self.signals = signals\n",
    "        self.incidenceVectors = dict()\n",
    "        self.referenceVector = np.array([])\n",
    "        self.incidenceDataFrame = None\n",
    "        self.correlationFactors = dict()\n",
    "\n",
    "    def getStartDate(self, d, region):\n",
    "        #d = the date at the start of our interval\n",
    "        # Finds the first available 'end_date' in the interval [d,Inf)\n",
    "        start_date = US_official[(US_official['end_date'] >= d) & (US_official['Site'] == region)]['end_date'].iloc[0]\n",
    "        return start_date\n",
    "\n",
    "\n",
    "    def getEndDate(self, d, region):\n",
    "        # Finds the last available 'end_date' in the interval (Inf,d]\n",
    "        end_date = US_official[(US_official['end_date'] <= d) & (US_official['Site'] == region)]['end_date'].iloc[-1]\n",
    "        return end_date\n",
    "\n",
    "\n",
    "    def calculateDaysBetween(self, date1, date2):\n",
    "        #returns the days between date1 and date2\n",
    "        try:\n",
    "            start_date = datetime.strptime(date1, '%Y-%m-%d')\n",
    "            end_date = datetime.strptime(date2, '%Y-%m-%d')\n",
    "            days = (end_date-start_date).days\n",
    "        except TypeError:\n",
    "            days = None\n",
    "        return days\n",
    "\n",
    "    def calculateVectorEntryForRegion(self, region, signal, start_date, end_date):\n",
    "        #Gets number of infections for \"region\" between specified dates, using the specified model \"signal\". These values are the elements of the incidence vectors\n",
    "        start_date = self.getStartDate(start_date,region)\n",
    "        end_date = self.getEndDate(end_date,region)\n",
    "        if signal in US_ML.columns:\n",
    "            data = US_ML[(US_ML['date'] >= start_date) & (US_ML['date'] <= end_date) & (US_ML['state code'] == region)]\n",
    "            cumulativeIntegral = data[signal].sum()\n",
    "        elif signal in US_cases.columns:\n",
    "            data = US_cases[(US_cases['submission_date'] >= start_date) & (US_cases['submission_date'] <= end_date) & (US_cases['state'] == region)]\n",
    "            cumulativeIntegral = data[signal].sum()\n",
    "        elif signal in WW_cases.columns:\n",
    "            data = WW_cases[(WW_cases['date'] >= start_date) & (WW_cases['date'] <= end_date) & (WW_cases['site'] == region)]\n",
    "            cumulativeIntegral = data[signal].sum()\n",
    "        return cumulativeIntegral\n",
    "\n",
    "    def MLEcoefs(self, reference_vector, incidence_matrix):\n",
    "        # Estimates the coefficients of Linear Regression\n",
    "        XtX = np.dot(incidence_matrix.T,incidence_matrix)\n",
    "        Xty = np.dot(incidence_matrix.T,reference_vector)\n",
    "        return np.linalg.solve(XtX,Xty)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TemporalIncidences(AcumulatedIncidences):\n",
    "\n",
    "    def __init__(self, region, interval_type='from0', signals=['p_cli','p_rf','p_XGB','p_glm','new_case','ww_cases'], rounds=None):\n",
    "        self.region = region\n",
    "        self.interval_type = interval_type\n",
    "        self.incidenceMatrix = np.empty((0,len(signals)+1))\n",
    "        if rounds is None:\n",
    "            self.rounds = list(US_official[US_official['Site']==self.region]['Round'])\n",
    "        else:\n",
    "            self.rounds = rounds\n",
    "        AcumulatedIncidences.__init__(self,signals)\n",
    "\n",
    "\n",
    "    def calculateVectors(self):\n",
    "        #Computes the incidence vectors: they're dicts where each pair of dates (start,end) has a value (that value is the % of infected people between those rounds and the selected region)\n",
    "        for start_date, end_date in self.zipRounds():\n",
    "            self.incidenceVectors['{0} {1}'.format(start_date, end_date)] = np.array([])\n",
    "            for signal in self.signals:\n",
    "                self.incidenceVectors['{0} {1}'.format(start_date, end_date)] = np.append(self.incidenceVectors['{0} {1}'.format(start_date, end_date)], self.calculateVectorEntryForRegion(self.region, signal, start_date, end_date))\n",
    "        return self.incidenceVectors\n",
    "\n",
    "    # single1 = Non-cumulative\n",
    "    # from1 = Cumulative\n",
    "    \n",
    "    def zipRounds(self):\n",
    "        #with this we can get the pairs of (start_date,end_date) with which we speccify the incidence vectors\n",
    "        dates = list(US_official[(US_official['Site'] == self.region) & (US_official['Round'].map(lambda x: x in self.rounds))]['end_date'])\n",
    "\n",
    "        if self.interval_type == 'single1':\n",
    "            return zip(dates[:-1], dates[1:])\n",
    "        elif self.interval_type == 'from1':\n",
    "            return zip([dates[0] for i in range(len(dates)-1)], dates[1:])\n",
    "        else:\n",
    "            raise ValueError('unknown type of interval has been used', self.interval_type)\n",
    "\n",
    "\n",
    "    def addReferenceVector(self):\n",
    "        self.referenceVector = np.array([])\n",
    "        for start_date, end_date in self.zipRounds():\n",
    "            cumulativeIntegral = US_official[(US_official['end_date'] == end_date) & (US_official['Site'] == self.region)]['Rate (%) [All Ages Cumulative Prevalence, Rounds 1-30 only]'].iloc[0]\n",
    "            self.referenceVector = np.append(self.referenceVector, cumulativeIntegral/100)\n",
    "        return self.referenceVector\n",
    "\n",
    "    def calculateIncidenceMatrix(self, normalize_everything=False):\n",
    "        self.calculateVectors()\n",
    "        self.incidenceMatrix = np.empty((0,len(self.signals)+1))\n",
    "        for start_date, end_date in self.zipRounds():\n",
    "            row = np.array([1]) # add a one if trying to add a coefficient\n",
    "            row = np.append(row, self.incidenceVectors['{0} {1}'.format(start_date, end_date)])\n",
    "            self.incidenceMatrix = np.vstack([self.incidenceMatrix, row])\n",
    "\n",
    "        # Normilise tot_cases and ww_cases (or everything):\n",
    "        to_normalize = self.signals if normalize_everything else ['new_case','ww_cases']\n",
    "        for signal in to_normalize:\n",
    "            if signal in self.signals:\n",
    "                col_i = self.signals.index(signal)+1\n",
    "                max_value = max(self.incidenceMatrix[:,col_i])\n",
    "                self.incidenceMatrix[:,col_i] = self.incidenceMatrix[:,col_i]/max_value\n",
    "\n",
    "        # Add days between data-points and real values:\n",
    "        if self.interval_type == 'from1':\n",
    "            # we have to remove the first column (constant)\n",
    "            self.incidenceMatrix = np.delete(self.incidenceMatrix,0,1)\n",
    "            a = np.array(US_official[(US_official['Site'] == self.region) & (US_official['Round'].map(lambda x: x in self.rounds))]['end_date'][1:].map(lambda x: datetime.strptime(x, '%Y-%m-%d')))\n",
    "            first = datetime.strptime(US_official[(US_official['Site'] == self.region) & (US_official['Round'].map(lambda x: x in self.rounds))]['end_date'].iloc[0], '%Y-%m-%d')\n",
    "            egunak = pd.array(a).map(lambda x: (x-first).days)\n",
    "            self.incidenceMatrix = np.column_stack([ self.incidenceMatrix, egunak/max(egunak) ]) # weeks between rounds (float1 interval)\n",
    "            self.incidenceMatrix = np.column_stack([ self.incidenceMatrix, np.array([US_official[(US_official['Site'] == self.region) & (US_official['Round'].map(lambda x: x in self.rounds))]['Rate (%) [All Ages Cumulative Prevalence, Rounds 1-30 only]'].iloc[0]/100 for i in range(len(a))]) ])\n",
    "\n",
    "        elif self.interval_type == 'single1':\n",
    "            a = np.array(US_official[(US_official['Site'] == self.region) & (US_official['Round'].map(lambda x: x in self.rounds))]['end_date'][1:].map(lambda x: datetime.strptime(x, '%Y-%m-%d')))\n",
    "            b = np.array(US_official[(US_official['Site'] == self.region) & (US_official['Round'].map(lambda x: x in self.rounds))]['end_date'][:-1].map(lambda x: datetime.strptime(x, '%Y-%m-%d')))\n",
    "            egunak = pd.array(a-b).map(lambda x: x.days)\n",
    "            self.incidenceMatrix = np.column_stack([ self.incidenceMatrix, egunak/max(egunak) ]) # days between rounds (normilised)\n",
    "            self.incidenceMatrix = np.column_stack([ self.incidenceMatrix, np.array(US_official[(US_official['Site'] == self.region) & (US_official['Round'].map(lambda x: x in self.rounds))]['Rate (%) [All Ages Cumulative Prevalence, Rounds 1-30 only]'][:-1])/100 ]) # zati 100 ehunekoa delako\n",
    "\n",
    "        return self.incidenceMatrix\n",
    "\n",
    "\n",
    "    def estimate(self, only_coefs=False, dates=False):\n",
    "        first_end_date = self.getStartDate('2020-01-01',self.region)\n",
    "        end_dates = [first_end_date] + [end for (start,end) in self.zipRounds()]\n",
    "\n",
    "        ref_vec = self.addReferenceVector()\n",
    "        inc_mat = self.calculateIncidenceMatrix()\n",
    "        coeff = self.MLEcoefs(ref_vec,inc_mat)\n",
    "        if only_coefs==True:\n",
    "            if dates==True:\n",
    "                return (coeff,end_dates)\n",
    "            else:\n",
    "                return coeff\n",
    "        else:\n",
    "            estim = np.dot(inc_mat,coeff)\n",
    "            if dates==True:\n",
    "                return (estim,end_dates)\n",
    "            else:\n",
    "                return estim\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AllRegionsAllRounds(AcumulatedIncidences):\n",
    "    \"\"\"\n",
    "    Works with multiple states at once. Used for nationwide models. The states used can be specified.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, interval_type='single1', states=None, signals=['p_cli','p_rf','p_XGB','p_glm','new_case','ww_cases']):\n",
    "        self.interval_type = interval_type\n",
    "        self.incidenceMatrix = np.empty((0,len(signals)+1))\n",
    "        AcumulatedIncidences.__init__(self,signals)\n",
    "        self.regions = regions if states==None else states\n",
    "\n",
    "    def zipRounds(self,region):\n",
    "        #with this we can get the pairs of (start_round,end_round) with which we speccify the incidence vectors\n",
    "        dates = list(US_official[US_official['Site'] == region]['end_date'])\n",
    "\n",
    "        if self.interval_type == 'single1':\n",
    "            return zip(dates[:-1], dates[1:])\n",
    "        elif self.interval_type == 'from1':\n",
    "            return zip([dates[0] for i in range(len(dates)-1)], dates[1:])\n",
    "        else:\n",
    "            raise ValueError('unknown type of interval has been used', self.interval_type)\n",
    "\n",
    "    def calculateIncidenceMatrix(self, normalize_everything=False):\n",
    "        self.incidenceMatrix = np.empty((0,len(self.signals)+3))\n",
    "        \n",
    "        # Normilise tot_cases and ww_cases (or everything):\n",
    "        to_normalize = self.signals if normalize_everything else ['new_case','ww_cases']\n",
    "        for region in self.regions:\n",
    "            temp_mat = np.empty((0,len(self.signals)+1))\n",
    "            for start_date, end_date in self.zipRounds(region):\n",
    "                row = np.array([1]) # add a one if trying to add an intercept\n",
    "                for signal in self.signals:\n",
    "                    row = np.append(row, self.calculateVectorEntryForRegion(region, signal, start_date, end_date))\n",
    "                temp_mat = np.vstack([temp_mat, row])\n",
    "            for signal in to_normalize:\n",
    "                if signal in self.signals:\n",
    "                    col_i = self.signals.index(signal)+1\n",
    "                    max_value = max(temp_mat[:,col_i])\n",
    "                    temp_mat[:,col_i] = temp_mat[:,col_i]/max_value\n",
    "                    \n",
    "            # Add real values and days between data-points:\n",
    "            a = np.array(US_official[US_official['Site']==region]['end_date'][1:])\n",
    "            b = np.array(US_official[US_official['Site']==region]['end_date'][:-1])\n",
    "            first = US_official[US_official['Site']==region]['end_date'].iloc[0]\n",
    "            if self.interval_type == 'from1':\n",
    "                egunak = np.array([self.calculateDaysBetween(first,xa) for xa in a])\n",
    "                refs = np.array([US_official[US_official['Site']==region]['Rate (%) [All Ages Cumulative Prevalence, Rounds 1-30 only]'].iloc[0]/100 for i in range(len(a))])\n",
    "            elif self.interval_type == 'single1':\n",
    "                egunak = np.array([self.calculateDaysBetween(xb,xa) for (xa,xb) in zip(a,b)])\n",
    "                refs = np.array([ xx/100 for xx in US_official[US_official['Site']==region]['Rate (%) [All Ages Cumulative Prevalence, Rounds 1-30 only]'][:-1] ])\n",
    "            temp_mat = np.column_stack([ temp_mat, egunak/egunak.max() ])\n",
    "            temp_mat = np.column_stack([ temp_mat, refs ])\n",
    "            self.incidenceMatrix = np.vstack([self.incidenceMatrix, temp_mat])\n",
    "            \n",
    "        if self.interval_type=='from1':\n",
    "            self.incidenceMatrix = np.delete(self.incidenceMatrix,0,1)\n",
    "\n",
    "\n",
    "        return self.incidenceMatrix\n",
    "\n",
    "    def addReferenceVector(self):\n",
    "        self.referenceVector = np.array([])\n",
    "        for region in self.regions:\n",
    "            for start_date, end_date in self.zipRounds(region):\n",
    "                cumulativeIntegral = US_official[(US_official['end_date'] == end_date) & (US_official['Site'] == region)]['Rate (%) [All Ages Cumulative Prevalence, Rounds 1-30 only]'].iloc[0]\n",
    "                self.referenceVector = np.append(self.referenceVector, cumulativeIntegral/100)\n",
    "        return self.referenceVector\n",
    "\n",
    "\n",
    "    def estimate(self, only_coefs=False, inc_mat=None):\n",
    "        ref_vec = self.addReferenceVector()\n",
    "        if type(inc_mat)==type(None):\n",
    "            inc_mat = self.calculateIncidenceMatrix()\n",
    "        coeff = self.MLEcoefs(ref_vec,inc_mat)\n",
    "        if only_coefs==True:\n",
    "            return coeff\n",
    "        else:\n",
    "            estim = np.dot(inc_mat,coeff)\n",
    "            return estim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ee2026",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4e7062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATEWIDE NN:\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch import nn\n",
    "import copy\n",
    "\n",
    "class nnExp(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(nnExp, self).__init__()\n",
    "    def forward(x):\n",
    "    x = torch.exp(x)\n",
    "        return x\n",
    "\n",
    "class nnLn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(nnLn, self).__init__()\n",
    "    def forward(x):\n",
    "        zrk = torch.zeros(x.shape)\n",
    "        x[x==zrk] = 1.0\n",
    "        x = torch.log(torch.abs(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class NN_2layers:\n",
    "\n",
    "    def __init__(self, state, interval, NN_type='relu', nodes_pl=6):\n",
    "        self.state = state\n",
    "        self.interval = interval\n",
    "        self.nn_type = NN_type\n",
    "        self.nodes_pl = nodes_pl\n",
    "\n",
    "        instance = TemporalIncidences(self.state,interval_type=self.interval)\n",
    "        self.helburua = instance.addReferenceVector()\n",
    "        self.datuak = instance.calculateIncidenceMatrix(normalize_everything=True)\n",
    "        if self.interval=='single1':\n",
    "            self.datuak = self.datuak[:,1:]\n",
    "        self.end_dates = instance.estimate(only_coefs=True,dates=True)[1]\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        def __init__(self, NN_type='relu', nodes_pl=6):\n",
    "            super().__init__()\n",
    "            self.nn_type = NN_type\n",
    "            self.nodes_pl = nodes_pl\n",
    "            self.hidden1 = nn.Linear(8,nodes_pl)\n",
    "            self.hidden2 = nn.Linear(nodes_pl,nodes_pl)\n",
    "            self.output = nn.Linear(nodes_pl,1)\n",
    "        def forward(self,x):\n",
    "            half_nodes = self.nodes_pl//2\n",
    "            third_nodes = self.nodes_pl//3\n",
    "            linear_out = self.hidden1(x)\n",
    "            if self.nn_type=='relu':\n",
    "                x = nn.functional.relu(linear_out)\n",
    "                linear_out = self.hidden2(x)\n",
    "                x = nn.functional.relu(linear_out)\n",
    "\n",
    "            elif self.nn_type=='mix':\n",
    "                nodes_1 = linear_out[:,0:third_nodes]\n",
    "                nodes_2 = linear_out[:,third_nodes:2*third_nodes]\n",
    "                nodes_3 = linear_out[:,2*third_nodes:]\n",
    "                x = torch.cat((nnExp.forward(nodes_1),nnLn.forward(nodes_2),nodes_3), dim=1)\n",
    "                linear_out = self.hidden2(x)\n",
    "                nodes_1 = linear_out[:,0:third_nodes]\n",
    "                nodes_2 = linear_out[:,third_nodes:2*third_nodes]\n",
    "                nodes_3 = linear_out[:,2*third_nodes:]\n",
    "                x = torch.cat((nnExp.forward(nodes_1),nnLn.forward(nodes_2),nodes_3), dim=1)\n",
    "\n",
    "            else:\n",
    "                raise ValueError('unknown type of NN has been used, accepted types are \"relu\", \"exp-ln\", \"ln-exp\", \"mix\".', self.interval_type)\n",
    "            x = self.output(x)\n",
    "            return x\n",
    "\n",
    "    def torcher(self,datuak,helburua):\n",
    "        self.X = torch.tensor(datuak, dtype=torch.float32)\n",
    "        y = torch.tensor(helburua, dtype=torch.float32)\n",
    "        self.y = y.reshape((-1,1))\n",
    "        return self.X, self.y\n",
    "\n",
    "    def batcher(self,batch_size, data_arrays=None, is_train=True):\n",
    "        if data_arrays==None:\n",
    "            data_arrays = (self.X,self.y)\n",
    "        dataset = data.TensorDataset(*data_arrays)\n",
    "        return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "    def initialise(self,learning_rate):\n",
    "        self.model = self.Model(NN_type=self.nn_type,nodes_pl=self.nodes_pl)\n",
    "        self.loss_function = nn.MSELoss()\n",
    "        self.optimiser = torch.optim.SGD(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def run_training(self, data_iter, num_epochs=5000, print_info=False):\n",
    "        jarrai = True\n",
    "        epoch = 0\n",
    "        set_loss = float(\"inf\")\n",
    "        since_loss_update = 0\n",
    "        while jarrai:\n",
    "            train_loss = []\n",
    "            self.model.train()\n",
    "            for data,target in data_iter:\n",
    "                self.optimiser.zero_grad()\n",
    "                output = self.model(data)\n",
    "                loss = self.loss_function(output,target)\n",
    "                loss.backward()\n",
    "                self.optimiser.step()\n",
    "                train_loss.append(loss.item())\n",
    "            if set_loss - np.mean(train_loss)>0.00001:\n",
    "                set_loss = np.mean(train_loss)\n",
    "                since_loss_update = 0\n",
    "            else:\n",
    "                since_loss_update += 1\n",
    "            if since_loss_update == num_epochs:\n",
    "                jarrai = False\n",
    "            epoch += 1\n",
    "            if print_info:\n",
    "                if epoch%1000==0:\n",
    "                    print(\"Epoch:\",epoch, \" Training Loss:\", np.mean(train_loss))\n",
    "        self.estimation = self.model(self.X).detach().numpy().T[0]\n",
    "        return self.estimation\n",
    "        \n",
    "    def ploter(self, print_MARE=True):\n",
    "        if print_MARE:\n",
    "            print(\"MARE:\",(np.abs(self.estimation-self.helburua)/self.helburua).mean())\n",
    "        plt.plot(self.end_dates[1:], self.helburua, marker=\".\", linestyle=\"\")\n",
    "        plt.plot(self.end_dates[1:], self.estimation)\n",
    "        plt.xticks(range(len(self.end_dates[1:])),rotation=90)\n",
    "        plt.show()\n",
    "\n",
    "    def reseter(self, reset_data=False):\n",
    "        self.features = None\n",
    "        self.labels = None\n",
    "        self.model = None\n",
    "        self.loss_function = None\n",
    "        self.optimiser = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        if reset_data:\n",
    "            self.helburua = None\n",
    "            self.datuak = None\n",
    "            self.end_dates = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be02287c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of NN gradient descent for non-cumulative CA (2 layer relu with 6 neurons per layer):\n",
    "\n",
    "state = 'CA'\n",
    "typeNN = 'relu'\n",
    "num_nodes = 6\n",
    "interval = 'single1'\n",
    "\n",
    "clcl = NN_2layers(state,interval,typeNN,num_nodes)\n",
    "\n",
    "X_data,y_data = clcl.torcher(clcl.datuak,clcl.helburua)\n",
    "trainloader = clcl.batcher(29, (X_data,y_data)\n",
    "\n",
    "clcl.initialise(0.001)# <- learning rate\n",
    "clcl.run_training(trainloader,5000, print_info=True)# <- num of epochs after which if there is no 0.00001 change it stops\n",
    "print(\"路MARE:\",(np.abs(clcl.estimation-clcl.helburua)/clcl.helburua).mean())\n",
    "\n",
    "estim_list=np.array(clcl.estimation)\n",
    "refvec_list=np.array(y_data).T[0]\n",
    "\n",
    "y_bar = [refvec_list.mean() for i in range(len(refvec_list))]\n",
    "SStot = np.linalg.norm(refvec_list - y_bar,2)**2\n",
    "SSres = np.linalg.norm(refvec_list - estim_list,2)**2\n",
    "print(\"路R^2:\",1 - SSres/SStot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ff1a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USA-level NN:\n",
    "\n",
    "class NN_2layersUSA:\n",
    "\n",
    "    def __init__(self, states, interval, NN_type='relu', nodes_pl=6):\n",
    "        self.states = states\n",
    "        self.interval = interval\n",
    "        self.nn_type = NN_type\n",
    "        self.nodes_pl = nodes_pl\n",
    "\n",
    "        instance = AllRegionsAllRounds(self.interval,self.states)\n",
    "        self.helburua = instance.addReferenceVector()\n",
    "        self.datuak = instance.calculateIncidenceMatrix(normalize_everything=True)\n",
    "        if self.interval == 'single1':\n",
    "                self.datuak = self.datuak[:,1:]\n",
    "        \n",
    "    class Model(nn.Module):\n",
    "        def __init__(self, NN_type='relu', nodes_pl=6):\n",
    "            super().__init__()\n",
    "            self.nn_type = NN_type\n",
    "            self.nodes_pl = nodes_pl\n",
    "            self.hidden1 = nn.Linear(8,nodes_pl)\n",
    "            self.hidden2 = nn.Linear(nodes_pl,nodes_pl)\n",
    "            self.output = nn.Linear(nodes_pl,1)\n",
    "        def forward(self,x):\n",
    "            half_nodes = self.nodes_pl//2\n",
    "            third_nodes = self.nodes_pl//3\n",
    "            linear_out = self.hidden1(x)\n",
    "            if self.nn_type=='relu':\n",
    "                x = nn.functional.relu(linear_out)\n",
    "                linear_out = self.hidden2(x)\n",
    "                x = nn.functional.relu(linear_out)\n",
    "\n",
    "            elif self.nn_type=='mix':\n",
    "                nodes_1 = linear_out[:,0:third_nodes]\n",
    "                nodes_2 = linear_out[:,third_nodes:2*third_nodes]\n",
    "                nodes_3 = linear_out[:,2*third_nodes:]\n",
    "                x = torch.cat((nnExp.forward(nodes_1),nnLn.forward(nodes_2),nodes_3), dim=1)\n",
    "                linear_out = self.hidden2(x)\n",
    "                nodes_1 = linear_out[:,0:third_nodes]\n",
    "                nodes_2 = linear_out[:,third_nodes:2*third_nodes]\n",
    "                nodes_3 = linear_out[:,2*third_nodes:]\n",
    "                x = torch.cat((nnExp.forward(nodes_1),nnLn.forward(nodes_2),nodes_3), dim=1)\n",
    "\n",
    "            else:\n",
    "                raise ValueError('unknown type of NN has been used, accepted types are \"relu\", \"mix\".', self.interval_type)\n",
    "            x = self.output(x)\n",
    "            return x\n",
    "\n",
    "    def torcher(self,datuak,helburua):\n",
    "        self.X = torch.tensor(datuak, dtype=torch.float32)\n",
    "        y = torch.tensor(helburua, dtype=torch.float32)\n",
    "        self.y = y.reshape((-1,1))\n",
    "        return self.X, self.y\n",
    "\n",
    "    def batcher(self,batch_size, data_arrays=None, is_train=True):\n",
    "        if data_arrays==None:\n",
    "            data_arrays = (self.X,self.y)\n",
    "        dataset = data.TensorDataset(*data_arrays)\n",
    "        return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "        \n",
    "    def initialise(self,learning_rate):\n",
    "        self.model = self.Model(NN_type=self.nn_type,nodes_pl=self.nodes_pl)\n",
    "        self.loss_function = nn.MSELoss()\n",
    "        self.optimiser = torch.optim.SGD(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def run_training(self, data_iter, num_epochs=5000, print_info=False):\n",
    "        jarrai = True\n",
    "        epoch = 0\n",
    "        set_loss = float(\"inf\")\n",
    "        since_loss_update = 0\n",
    "        while jarrai:\n",
    "            train_loss = []\n",
    "            self.model.train()\n",
    "            for data,target in data_iter:\n",
    "                self.optimiser.zero_grad()\n",
    "                output = self.model(data)\n",
    "                loss = self.loss_function(output,target)\n",
    "                loss.backward()\n",
    "                self.optimiser.step()\n",
    "                train_loss.append(loss.item())\n",
    "            if set_loss - np.mean(train_loss)>0.00001:\n",
    "                set_loss = np.mean(train_loss)\n",
    "                since_loss_update = 0\n",
    "            else:\n",
    "                since_loss_update += 1\n",
    "            if since_loss_update == num_epochs:\n",
    "                jarrai = False\n",
    "            epoch += 1\n",
    "            if print_info:\n",
    "                if epoch%1000==0:\n",
    "                    print(\"Epoch:\",epoch, \" Training Loss:\", np.mean(train_loss))\n",
    "        self.estimation = self.model(self.X).detach().numpy().T[0]\n",
    "        return self.estimation\n",
    "\n",
    "    def reseter(self, reset_data=False):\n",
    "        self.features = None\n",
    "        self.labels = None\n",
    "        self.model = None\n",
    "        self.loss_function = None\n",
    "        self.optimiser = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        if reset_data:\n",
    "            self.helburua = None\n",
    "            self.datuak = None\n",
    "            self.end_dates = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6a1f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of NN gradient descent for cumulative Top10 states (2 layer mixed with 6 neurons per layer):\n",
    "\n",
    "top10 = ['CA','TX','FL','GA','NY','PA','IL','OH','MI','NC']\n",
    "states = top10\n",
    "typeNN = 'mix'\n",
    "num_nodes = 6\n",
    "interval = 'from1'\n",
    "\n",
    "clcl = NN_2layersUSA(states,interval,typeNN,num_nodes)\n",
    "\n",
    "X_data,y_data = clcl.torcher(clcl.datuak,clcl.helburua)\n",
    "trainloader = clcl.batcher(clcl.datuak.shape[0], (X_data,y_data))\n",
    "\n",
    "clcl.initialise(0.001)# <- learning rate\n",
    "clcl.run_training(trainloader,5000, print_info=False)\n",
    "\n",
    "print(\"路MARE:\",(np.abs(clcl.estimation-clcl.helburua)/clcl.helburua).mean())\n",
    "\n",
    "estim_list=np.array(clcl.estimation)\n",
    "refvec_list=np.array(y_data).T[0]\n",
    "\n",
    "y_bar = [refvec_list.mean() for i in range(len(refvec_list))]\n",
    "SStot = np.linalg.norm(refvec_list - y_bar,2)**2\n",
    "SSres = np.linalg.norm(refvec_list - estim_list,2)**2\n",
    "print(\"路R^2:\",1 - SSres/SStot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
